ğŸ“„ RAG Chatbot (Advanced & Free)

A Retrieval-Augmented Generation (RAG) chatbot that answers questions from your documents using hybrid retrieval, reranking, and a local LLM â€” completely free, offline, and no API keys required.



ğŸš€ Features

ğŸ“‚ PDF document ingestion

ğŸ§  Semantic search using embeddings

ğŸ” Hybrid retrieval (FAISS + BM25)

ğŸ¯ Reranking using cross-encoder models

ğŸ¤– Local LLM inference using Ollama

âš¡ Optimized for speed with caching & conditional execution

ğŸŒ FastAPI backend

ğŸ¨ Streamlit frontend UI

ğŸ” No paid APIs, runs fully locally







ğŸ—ï¸ Project Architecture

User
 â†“
Streamlit UI
 â†“
FastAPI Backend (/ask)
 â†“
Query Expansion (optional)
 â†“
Hybrid Retrieval (BM25 + FAISS)
 â†“
Reranking (Cross-Encoder)
 â†“
Local LLM (Ollama - llama3)
 â†“
Final Answer




ğŸ“ Folder Structure
RAG_CHATBOT/
â”œâ”€â”€ api.py                  # FastAPI backend
â”œâ”€â”€ ui.py                   # Streamlit frontend
â”œâ”€â”€ ingest.py               # PDF ingestion & embedding creation
â”œâ”€â”€ rag_chain.py            # Core RAG pipeline (optimized)
â”‚
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ local_llm.py        # Ollama LLM loader (cached)
â”‚
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ query_expansion.py  # Smart query expansion
â”‚
â”œâ”€â”€ retrievers/
â”‚   â”œâ”€â”€ hybrid.py           # BM25 + FAISS retrieval
â”‚   â””â”€â”€ reranker.py         # Cross-encoder reranking
â”‚
â”œâ”€â”€ data/                   # Input PDFs
â”œâ”€â”€ embeddings/             # FAISS vector store
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md






RAG_CHATBOT/
â”œâ”€â”€ api.py                  # FastAPI backend
â”œâ”€â”€ ui.py                   # Streamlit frontend
â”œâ”€â”€ ingest.py               # PDF ingestion & embedding creation
â”œâ”€â”€ rag_chain.py            # Core RAG pipeline (optimized)
â”‚
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ local_llm.py        # Ollama LLM loader (cached)
â”‚
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ query_expansion.py  # Smart query expansion
â”‚
â”œâ”€â”€ retrievers/
â”‚   â”œâ”€â”€ hybrid.py           # BM25 + FAISS retrieval
â”‚   â””â”€â”€ reranker.py         # Cross-encoder reranking
â”‚
â”œâ”€â”€ data/                   # Input PDFs
â”œâ”€â”€ embeddings/             # FAISS vector store
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md






ğŸ§  How It Works (Simple Explanation)

Documents are ingested

PDFs are split into chunks

Chunks are converted into embeddings

Stored in FAISS vector database

User asks a question

Short queries may be expanded

Hybrid retrieval fetches relevant chunks

Reranker selects best chunks

LLM generates final answer using retrieved context

Optimizations

Models are loaded once

Reranking & expansion are used conditionally

Context size is limited for faster inference





ğŸ› ï¸ Installation & Setup


1ï¸âƒ£ Clone the repository
git clone https://github.com/your-username/RAG_CHATBOT.git
cd RAG_CHATBOT



2ï¸âƒ£ Create & activate virtual environment
python -m venv .venv
.venv\Scripts\activate   # Windows


3ï¸âƒ£ Install dependencies
pip install -r requirements.txt



4ï¸âƒ£ Install & run Ollama
ollama pull llama3
ollama run llama3


ğŸ“¥ Ingest Documents

Place your PDFs inside the data/ folder, then run:
python ingest.py



â–¶ï¸ Run the Application
Terminal 1 â€” Backend
uvicorn api:app

Terminal 2 â€” Frontend
streamlit run ui.py
